# columbia_adl_a3

## 3b.
Several optimzers (SGD, SGD with momentum, Adagrad, Adam) with a fixed learning rate (fast enough and still decreasing the loss) were tested for their performance. We observed that Adam, which combines both momentum and adaptive learning algorithms, performs the best. It was also demonstrated that AdaGrad accelerates the convergence by accelerating per parameter learning.
